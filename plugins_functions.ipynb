{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXnd1cCu1TkJ"
      },
      "source": [
        "# Add Plugins Step by Step - using PlugnPlai and LangChain\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/edreisMD/plugnplai/blob/main/examples/plugins_step_by_step.ipynb)\n",
        "\n",
        "The goal of this example is to go through all the steps to add plugins to LLMs\n",
        "1. Get plugins of certain categories from [plugnplai.com](https://plugnplai.com)\n",
        "2. Load plugins manifest and specifications\n",
        "3. Parse specifications and generate a prompt with the descriptions\n",
        "4. Use [LangChain]() to call the LLM\n",
        "5. Parse the LLM response, looking for the `[API]` pattern defined on `plugins.prompt`\n",
        "6. Call the plugin using `plugins.call()`\n",
        "7. Use LangChain again to ask the LLM a final response using the new data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZWQil8RrAKV"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RUcr3tLEK7M",
        "outputId": "1d23a54a-d77d-4090-97a9-108d2870cae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.3/756.3 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install plugnplai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fYY9i4Z3sBO"
      },
      "source": [
        "# Get the plugins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlC-9wyp2vtF"
      },
      "source": [
        "We want to install at maximum three plugins, in order to fit the description on the context length\n",
        "\n",
        "Lets find one plugin for each category:\n",
        "1. travel\n",
        "2. shopping\n",
        "3. weather\n",
        "\n",
        "We can use PlugnPlai categories (see [API reference](https://plugnplai.github.io/))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmqqHyg5Eee0",
        "outputId": "f4d877f4-daeb-4701-e24d-1721fb07cb23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our chosen Plugins: ['https://klarna.com']\n"
          ]
        }
      ],
      "source": [
        "import plugnplai as pl\n",
        "# Get working plugins - only tested plugins (in progress)\n",
        "allUrls = pl.get_plugins()\n",
        "\n",
        "# Lets pick Trip, Klarna and Speak\n",
        "urls = [plugin for plugin in allUrls if 'klarna' in plugin]\n",
        "\n",
        "print(f'Our chosen Plugins: {urls}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj5MtCaPyk9p"
      },
      "source": [
        "# Load and activate the plugins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JMHi1UTpy8LV"
      },
      "outputs": [],
      "source": [
        "from plugnplai import Plugins\n",
        "\n",
        "plugins = Plugins.install_and_activate(urls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-alHvKpyk9q"
      },
      "source": [
        "## Show the function object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g770AOGZ0vwH",
        "outputId": "eaf4c814-518f-4fd5-b299-8c0a83d31322"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'name': 'KlarnaProducts__opid__productsUsingGET',\n",
              "  'description': 'Assistant uses the Klarna plugin to get relevant product suggestions for any shopping or product discovery purpose.',\n",
              "  'parameters': {'countryCode': {'type': 'string',\n",
              "    'description': 'ISO 3166 country code with 2 characters based on the user location. Currently, only US, GB, DE, SE and DK are supported.',\n",
              "    'required': True},\n",
              "   'q': {'type': 'string',\n",
              "    'description': \"A precise query that matches one very small category or product that needs to be searched for to find the products the user is looking for. If the user explicitly stated what they want, use that as a query. The query is as specific as possible to the product name or category mentioned by the user in its singular form, and don't contain any clarifiers like latest, newest, cheapest, budget, premium, expensive or similar. The query is always taken from the latest topic, if there is a new topic a new query is started. If the user speaks another language than English, translate their request into English (example: translate fia med knuff to ludo board game)!\",\n",
              "    'required': True},\n",
              "   'size': {'type': 'integer',\n",
              "    'description': 'number of products returned',\n",
              "    'required': False},\n",
              "   'min_price': {'type': 'integer',\n",
              "    'description': \"(Optional) Minimum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.\",\n",
              "    'required': False},\n",
              "   'max_price': {'type': 'integer',\n",
              "    'description': \"(Optional) Maximum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.\",\n",
              "    'required': False}}}]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "plugins.functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "plugins.functions[0][\"description\"] = \"Assistant uses the Klarna plugin to get relevant product suggestions for any shopping or product discovery purpose.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z_WvLZ5yk9q"
      },
      "source": [
        "## Lets look at the length of the prompt\n",
        "\n",
        "Get the number of tokens of the prompt by just calling 'plugins.tokens'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnFld3xszdrx",
        "outputId": "04c0bfc3-45bd-471e-8519-f8cee2bac075"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "573\n"
          ]
        }
      ],
      "source": [
        "print(plugins.func_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikGFVxvPyk9q"
      },
      "source": [
        "## Call the LLM with functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "QTKMtRiMyk9r"
      },
      "outputs": [],
      "source": [
        "# You will need to first define your API key\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1C-RZvUyk9r"
      },
      "source": [
        "#### Uncomment or modify the message to test different plugins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "EFc4Xw3Dyk9r"
      },
      "outputs": [],
      "source": [
        "# Test Klarna Plugin\n",
        "HUMAN_MESSAGE = \"I want to buy a rolling stones t-shirt\"\n",
        "\n",
        "# Test Trip Plugin\n",
        "# HUMAN_MESSAGE = \"I need a hotel in Paris between Dec.3-8\"\n",
        "\n",
        "# Test Speak Plugin\n",
        "# HUMAN_MESSAGE = \"How to say I love you in Portuguese?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-JDpX0Uyk9r"
      },
      "source": [
        "#### Call LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S5B0ibJyk9r",
        "outputId": "08049290-519e-45a3-b750-cf8b983f5281"
      },
      "outputs": [
        {
          "ename": "InvalidRequestError",
          "evalue": "Invalid schema for function 'KlarnaProducts__opid__productsUsingGET': schema must be a JSON Schema of 'type: \"object\"', got 'type: \"None\"'.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[47], line 62\u001b[0m\n\u001b[1;32m     55\u001b[0m         second_response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m     56\u001b[0m             model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-4\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     57\u001b[0m             messages\u001b[39m=\u001b[39mmessages,\n\u001b[1;32m     58\u001b[0m         )  \u001b[39m# get a new response from GPT where it can see the function response\u001b[39;00m\n\u001b[1;32m     59\u001b[0m         \u001b[39mreturn\u001b[39;00m second_response\n\u001b[0;32m---> 62\u001b[0m \u001b[39mprint\u001b[39m(run_conversation())\n",
            "Cell \u001b[0;32mIn[47], line 28\u001b[0m, in \u001b[0;36mrun_conversation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m# Use 'plugins.functions' on the gpt call \u001b[39;00m\n\u001b[1;32m     26\u001b[0m functions \u001b[39m=\u001b[39m plugins\u001b[39m.\u001b[39mfunctions\n\u001b[0;32m---> 28\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     29\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-4\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     30\u001b[0m     messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[1;32m     31\u001b[0m     functions\u001b[39m=\u001b[39;49mfunctions,\n\u001b[1;32m     32\u001b[0m     \u001b[39m# auto is default, but here we will force GPT to call the function\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m     function_call\u001b[39m=\u001b[39;49m {\u001b[39m\"\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mKlarnaProducts__opid__productsUsingGET\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     36\u001b[0m response_message \u001b[39m=\u001b[39m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     38\u001b[0m \u001b[39m# Step 2: check if GPT wanted to call a function\u001b[39;00m\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/plugnplai-nyjYP0nS-py3.10/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/plugnplai-nyjYP0nS-py3.10/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/plugnplai-nyjYP0nS-py3.10/lib/python3.10/site-packages/openai/api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    220\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    221\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    222\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 230\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/plugnplai-nyjYP0nS-py3.10/lib/python3.10/site-packages/openai/api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    617\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    618\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    619\u001b[0m         )\n\u001b[1;32m    620\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    621\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 624\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    625\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    626\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    627\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    628\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    629\u001b[0m         ),\n\u001b[1;32m    630\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    631\u001b[0m     )\n",
            "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/plugnplai-nyjYP0nS-py3.10/lib/python3.10/site-packages/openai/api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    686\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    688\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    689\u001b[0m     )\n\u001b[1;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: Invalid schema for function 'KlarnaProducts__opid__productsUsingGET': schema must be a JSON Schema of 'type: \"object\"', got 'type: \"None\"'."
          ]
        }
      ],
      "source": [
        "import openai\n",
        "import json\n",
        "\n",
        "def use_call_api(response_message):\n",
        "    \n",
        "    function_name = response_message[\"function_call\"][\"name\"]\n",
        "    split_name = function_name.split(\"__opid__\")\n",
        "    plugin_name = split_name[0]\n",
        "    operation_id = split_name[1]\n",
        "    parameters = response_message[\"function_call\"][\"arguments\"]\n",
        "    \n",
        "    r = plugins.call_api(plugin_name = plugin_name,\n",
        "                        operation_id = operation_id,\n",
        "                        parameters = parameters\n",
        "                        )\n",
        "\n",
        "    api_response = r.json()\n",
        "    return r.json()\n",
        "\n",
        "\n",
        "def run_conversation():\n",
        "    # Step 1: send the conversation and available functions to GPT\n",
        "    messages = [{\"role\": \"user\", \"content\": HUMAN_MESSAGE}]\n",
        "    \n",
        "    # Use 'plugins.functions' on the gpt call \n",
        "    functions = plugins.functions\n",
        "    \n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=messages,\n",
        "        functions=functions,\n",
        "        # auto is default, but here we will force GPT to call the function\n",
        "        function_call= {\"name\": \"KlarnaProducts__opid__productsUsingGET\"},\n",
        "    )\n",
        "    \n",
        "    response_message = response[\"choices\"][0][\"message\"]\n",
        "\n",
        "    # Step 2: check if GPT wanted to call a function\n",
        "    if response_message.get(\"function_call\"):\n",
        "        # Step 3: call the function\n",
        "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
        "        \n",
        "        # now use plugins.call_api defined on the use_call_api above\n",
        "        function_response = use_call_api(response_message)\n",
        "\n",
        "        # Step 4: send the info on the function call and function response to GPT\n",
        "        messages.append(response_message)  # extend conversation with assistant's reply\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"function\",\n",
        "                \"name\": response_message[\"function_call\"][\"name\"],\n",
        "                \"content\": function_response,\n",
        "            }\n",
        "        )  # extend conversation with function response\n",
        "        second_response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=messages,\n",
        "        )  # get a new response from GPT where it can see the function response\n",
        "        return second_response\n",
        "\n",
        "\n",
        "print(run_conversation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6dRUL_iyk9r"
      },
      "source": [
        "## Parse the LLM response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g86d1vRVyk9r",
        "outputId": "1c5c0712-29cc-4df2-afa0-8a00b63be6cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'plugin_name': 'Trip', 'operation_id': 'search_hotel', 'parameters': {'cityName': 'Paris', 'checkIn': '2023-12-03', 'checkOut': '2023-12-08', 'topHotel': 5, 'locale': 'en', 'starList': [], 'facilityList': [], 'themeList': [], 'typeList': [], 'originalInput': 'I need a hotel in Paris between Dec.3-8', 'originalInputInEnglish': 'I need a hotel in Paris between Dec.3-8'}}\n"
          ]
        }
      ],
      "source": [
        "# import the parser function\n",
        "from plugnplai import parse_llm_response\n",
        "\n",
        "# Parse the LLM response importing '\n",
        "call_dict = parse_llm_response(llm_first_response)\n",
        "print(call_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFJ0P-WXyk9r"
      },
      "source": [
        "## Call Plugin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts_SUlHzyk9r"
      },
      "source": [
        "## LLM responds using the API data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "faIEWpfEyk9r",
        "outputId": "09e69c09-1026-448a-aa40-dbe54ce5ed7e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Here are some hotels in Paris available from December 3rd to December 8th:\n",
              "\n",
              "1. [Novotel Paris les Halles](https://us.trip.com/hotels/detail/?cityId=192&hotelId=2196505&checkin=2023-12-03&checkout=2023-12-08&curr=USD&allianceid=3842389&sid=22086800)\n",
              "   - Price: $394 USD\n",
              "   - Address: 8 Pl. Marguerite de Navarre\n",
              "   - Score: 4.6/5.0\n",
              "   - Features: Children's playground, Gym\n",
              "\n",
              "2. [OKKO Hotels Paris Gare de l'Est](https://us.trip.com/hotels/detail/?cityId=192&hotelId=33577969&checkin=2023-12-03&checkout=2023-12-08&curr=USD&allianceid=3842389&sid=22086800)\n",
              "   - Price: $170 USD\n",
              "   - Address: 30A Rue d'Alsace\n",
              "   - Score: 4.4/5.0\n",
              "   - Features: Sauna, Conference hall\n",
              "\n",
              "3. [25Hours Hotel Terminus Nord](https://us.trip.com/hotels/detail/?cityId=192&hotelId=23227512&checkin=2023-12-03&checkout=2023-12-08&curr=USD&allianceid=3842389&sid=22086800)\n",
              "   - Price: $181 USD\n",
              "   - Address: 12 Bd de Denain\n",
              "   - Score: 4.3/5.0\n",
              "   - Features: Conference hall, Laundry room\n",
              "\n",
              "4. [Maison Mère](https://us.trip.com/hotels/detail/?cityId=192&hotelId=717696&checkin=2023-12-03&checkout=2023-12-08&curr=USD&allianceid=3842389&sid=22086800)\n",
              "   - Price: $190 USD\n",
              "   - Address: 7 Rue Mayran\n",
              "   - Score: 4.7/5.0\n",
              "   - Features: Conference hall, Business center\n",
              "\n",
              "5. [The Originals Boutique, Hotel Maison Montmartre, Paris](https://us.trip.com/hotels/detail/?cityId=192&hotelId=17507189&checkin=2023-12-03&checkout=2023-12-08&curr=USD&allianceid=3842389&sid=22086800)\n",
              "   - Price: $114 USD\n",
              "   - Address: 32 Av. de la Prte de Montmartre\n",
              "   - Score: 4.1/5.0\n",
              "   - Features: Business center, Suites\n",
              "\n",
              "Please note that prices and availability are subject to change."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "api_return_prompt = f\"\"\"\n",
        "Assistant is a large language model with access to plugins.\n",
        "\n",
        "Assistant called a plugin in response to this human message:\n",
        "# HUMAN MESSAGE\n",
        "{HUMAN_MESSAGE}\n",
        "\n",
        "# API REQUEST SUMMARY\n",
        "{llm_first_response}\n",
        "\n",
        "# API RESPONSE\n",
        "{api_response}\n",
        "\"\"\"\n",
        "\n",
        "# Install the plugins ewith the original template\n",
        "plugins = Plugins.install_and_activate(urls)\n",
        "\n",
        "chat = ChatOpenAI(temperature=0, model=\"gpt-4-0613\")\n",
        "# chat = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=api_return_prompt),\n",
        "    HumanMessage(content=\"HUMAN_MESSAGE\")\n",
        "]\n",
        "\n",
        "res = chat(messages)\n",
        "\n",
        "display(Markdown(res.content))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "plugnplai-nyjYP0nS-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
